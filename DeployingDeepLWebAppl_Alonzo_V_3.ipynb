{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LonzoBonzo/AI-Machine-Learning-Web-Development-/blob/main/DeployingDeepLWebAppl_Alonzo_V_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Techniques Employed\n",
        "\n",
        "These are some of the technologies and concepts you will become familiar with in this project. We provide detailed instructions and references to aid in completing each item. These tools will expand your data science skillset. The key to being a successful data scientist isn’t knowing every tool, but rather being flexible as you explore and work with new technologies in this ecosystem.\n",
        "\n",
        "    Colab: Train deep learning models using GPU\n",
        "\n",
        "    TensorFlow/Keras: Open source libraries to perform deep learning\n",
        "\n",
        "    TensorFlow.js: Use the TensorFlow ecosystem to deploy the deep learning model\n",
        "\n",
        "    Expo/ReactNative: Create the web application\n",
        "\n",
        "    Heroku/GitHub Pages: Create the web application\n",
        "\n",
        "    Docker: Deploy the web application\n",
        "\n",
        "    Expo: Deploy the mobile application\n",
        "\n",
        "    Mobile phone\n",
        "\n",
        "    Node.js/NVM: Deploy the mobile application"
      ],
      "metadata": {
        "id": "dWvSLqHlDcx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Outline\n",
        "\n",
        "The project consists of 3 milestones. As each milestone emphasizes different skills, the deliverables for each will vary.\n",
        "\n",
        "1. Build a Food Image Classifier:\n",
        "\n",
        "You will perform deep learning image classification using pre-trained models and experiment with some fine-tuning. You will experiment with models like ResNet and mobile-optimized models like MobileNet. You will learn to evaluate a model on its inference, size, and accuracy.\n",
        "\n",
        "Deliverables\n",
        "\n",
        "    Colab notebook\n",
        "    Table comparing metrics on different model architectures\n",
        "\n",
        "2. Deploy the Model to the Web\n",
        "\n",
        "Given the trained model from Milestone 1, you will optimize the model for serving on the web.\n",
        "\n",
        "There are 2 ways to deploy an app:\n",
        "\n",
        "    running on the server\n",
        "    running on the browser natively\n",
        "\n",
        "For the server deployment, you will learn how to deploy the model as a service using Docker, and run on Heroku.\n",
        "\n",
        "For the browser, you will serve the model using TensorFlow.js as a static site on GitHub pages.\n",
        "\n",
        "Deliverables\n",
        "\n",
        "    Link to Heroku web application\n",
        "    Link to GitHub repo\n",
        "\n",
        "3. Deploy the App to Mobile\n",
        "\n",
        "Convert the model to run on your mobile device using Expo. You will take the model trained from Milestone 1 and convert it to a format needed for TensorFlow.js-native. You will be provided with boilerplate code to use the transformed model in a React Native project. Expo is a cross-platform mobile development library for React Native.\n",
        "\n",
        "Deliverables\n",
        "\n",
        "    Link to GitHub repo\n",
        "    Screenshot of mobile application\n",
        "\n",
        "These skills are covered in the following order:\n",
        "\n",
        "    Train a deep learning model using TensorFlow\n",
        "    Export that model\n",
        "    Optimize the model for latency\n",
        "    Deploy the model to both web and mobile platforms\n",
        "\n",
        "Each section builds upon the previous one and will expand your skillset in both data science and data engineering.\n",
        "\n",
        "As you go through the project, keep in mind the overall objective: **Train an image classifier using deep learning, and deploy the model.**"
      ],
      "metadata": {
        "id": "ZNi1D8RLD_Rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks Deliverable 1 for Week Jan 29 - Feb 2\n",
        "\n",
        "## Explain the following concepts:\n",
        "\n",
        "1. Declaring Variables: Declaring varibles is the process of defining them within a program and script.\n",
        "\n",
        "2. Importing data into pandas DataFrames:Involves reading data from an external sources such as CSV files, Excle files, SQL databases, or other formats and loading that data into a Datafram object provided by the pandas library in Python.\n",
        "\n",
        "3. Manipulating datetime variables: Involves performing various operations on date and time values, such as extracting components, arithmetic operation, formatting dates, and more.\n",
        "\n",
        "4. Using Indices in DataFrames:Crucial for efficient data manipulation and retrieval. Provides a way to label and reference rows, similar to row labels in a database or spreadsheet.\n",
        "\n",
        "5. Plotting using matplotlib or seaborn: Used for creating visualizations in Python. Matplotlib is a powerful library for creating static, interactive, and animated visualizations, while Seaborn is built on top of Matplotlib and provides a higher-level interface for creating attractive statistical graphics.\n",
        "\n",
        "6. Learning new predefined functions in Python libraries:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o2cD3cv-FQaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "The Food 101 data is used for this project, which includes 101 food categories for a total of 101,000 images. Thus, each class has 1,000 images, of which 250 are manually reviewed test images, and 750 are training images. The categories of the ETHZ Food 101 are the 101 most popular categories from the food picture sharing website foodspotting.com. The labels of food categories were chosen from the top 101 most popular dishes.\n",
        "\n",
        "**Data citation**\n",
        "\n",
        "Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc, Food 101 Mining Discriminative Components with Random Forests, European Conference on Computer Vision, 2014."
      ],
      "metadata": {
        "id": "KusOl2XsFqmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks Deliverable 2 for Week Jan 29 - Feb 2\n",
        "\n",
        "1. Find three papers related to food image recognition. Search at : [Google Scholar](https://scholar.google.com///)\n",
        "\n",
        "2. Write here below the title of the three chosen papers:\n",
        "\n",
        "paper 1: FoodAI: Food Image Recognition via Deep Learning for Smart Food Logging\n",
        "\n",
        "paper 2: Automated Food image Classification using Deep Learning approach\n",
        "\n",
        "paper 3: Food image recognition using deep convolutional network with pre-training and fine-tuning\n",
        "\n",
        "\n",
        "3. Use [Explainpaper AI tool](https://www.explainpaper.com/) to highlight confusing text and get an explanation.\n",
        "\n",
        "4. Write here below what you found interesting about each paper after using explainpaper AI tool.\n",
        "\n",
        "paper 1: I found it interesting that you could use this in a lot of pratical ways. You could use this for fitness goals and writing down everything down can be a hassle so coming up with a FoodAI is like having an assistant on your phone.\n",
        "\n",
        "paper 2: I find it interesting about SqueezeNet and VGG-16 both having different accuracies because one is faster than the other but even then the accuracy is only about a 9% difference.\n",
        "\n",
        "paper 3:By the third paper I was pretty familiar with the deep learning method.\n"
      ],
      "metadata": {
        "id": "mcAjfOrFGGf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective\n",
        "\n",
        "Use transfer learning and TensorFlow 2 to train at least 2 models on the Food 101 data set. The purpose is to get experience using a pre-trained model and learn how to evaluate models based on accuracy, number of parameters, training time, and model size."
      ],
      "metadata": {
        "id": "KO_Kzs-PGggS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importance to project\n",
        "\n",
        ">The output from the image classifier you’ll train in Milestone 1 is a model file, which will be used as input to Milestones 2 and 3, to deploy the web and mobile applications.\n",
        "\n",
        ">Transfer learning is an important technique in deep learning, as it allows you to use state-of-the-art models trained on large data sets and optimize them for your problem.\n",
        "\n",
        ">Not all model architectures are optimized for every use case, such as reduced latency versus highest accuracy.\n",
        "\n",
        ">In this milestone, we will evaluate a regular model like VGG19 vs. a mobile-optimized model, such as MobileNetV2. Then, we’ll evaluate them on criteria like accuracy, model size, and time.\n",
        "\n",
        ">Our goal in this project is to create web and mobile applications. We will try different architectures and models, compare them, and choose a model that works best for deployment in terms of speed and size."
      ],
      "metadata": {
        "id": "xS5txMe3HDnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subsetting the data\n",
        "\n",
        "The full dataset has 101 classes (5GB), which will take some time to run. You will want to subset your data for a couple of reasons:\n",
        "\n",
        "1. You can reduce training time while experimenting and tweaking the code. This is a good practice in any data science project.\n",
        "\n",
        "2. You can save computing costs by reducing GPU time. In practice, you or your company would be paying for a GPU server and training on a smaller data set while experimenting saves costs.\n",
        "\n",
        "Begin by training three classes. Once the code is closer to completion, you can increase the number of classes of the data set."
      ],
      "metadata": {
        "id": "hAq7p6OMHbF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow\n",
        "\n",
        "1. Set up your development environment with a GPU. Google Colab is a free option that is recommended.\n",
        "\n",
        "2. Download the subset of the Food-101 dataset.\n",
        "\n",
        "The subset is composed of 3 classes (apple_pie, caesar_salad, falafel). The size of the file is 154MB.\n",
        "\n",
        "3. There are a number of pre-trained models available in TensorFlow 2 / Keras. Explore these available models:\n",
        "\n",
        "  >small (VGG19)\n",
        "\n",
        "  >intermediate (ResNet50)\n",
        "\n",
        "  >mobile-friendly (MobileNetV2)\n",
        "\n",
        "Look at the size of the model, its expected training accuracy, depth, etc.\n",
        "\n",
        "4. Split your data into training and validation using tf.keras.preprocessing.image.ImageDataGenerator\n",
        "\n",
        "5. Train a large model family, such as VGG19/ResNet50, using pre-trained ImageNet weights.\n",
        "\n",
        "6. Train a mobile-friendly model family like MobileNet using pre-trained ImageNet weights.\n",
        "\n",
        "7. Save the model and classes. There are various formats to save the model and list of classes. We saved the model in h5 format, giving it the name model.h5. We saved the list of classes as a JSON file with the name classes.json.\n",
        "\n",
        "**These model and classes files will be used as input for Milestone 2 when creating the web app.**"
      ],
      "metadata": {
        "id": "cBJXKOxmH1rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks Deliverable 3 for Week Jan 29 - Feb 2\n",
        "\n",
        "Based on the workflow section and without look at the dataset yet, answer the following questions:\n",
        "\n",
        "1. How does the choice of pre-trained model architecture (e.g., VGG19, ResNet50, MobileNetV2) impact the training accuracy and model size when applied to the Food-101 dataset subset (apple_pie, caesar_salad, falafel)? Are there significant differences in terms of depth, training time, and computational resources required for each of these models?\n",
        "\n",
        "answer here: When picking a pre trained model for the Food-101, it's cruicial to find a sweet spot between accuracy, model size, complexity, training time, and the amount of computer power you have. A lighter model like MobileNetV2 might be just right.\n",
        "\n",
        "2. What is the effectiveness of using transfer learning with pre-trained ImageNet weights when training large model families (e.g., VGG19, ResNet50) on the Food-101 dataset subset compared to training them from scratch? How does this effectiveness compare to training mobile-friendly model families like MobileNetV2 using pre-trained weights? Are there any trade-offs between model performance and resource utilization?\n",
        "\n",
        "answer here: Using pre-trained ImageNet weights for transfer learning is really helpful for bigger and smaller models on the Food101 dataset. Bigger models need more computer power, so it's important to think about how much and what the project needs.\n",
        "\n",
        "3. In the context of setting up a development environment with GPU support using Google Colab, what are the challenges and considerations when handling a dataset of size 154MB, such as the Food-101 dataset subset? How does the choice of data splitting method using tf.keras.preprocessing.image.ImageDataGenerator impact model training and validation?\n",
        "\n",
        "answer here: When using Google Colab with a dataset like Food-101 (154MB), you may face challenges like limited storage and memory. Choosing how to split your data using ImageDataGenerator can affect how well your model learns. For example, splitting your data randomly might not give your model enough examples of each type of food to learn from, while splitting it with ImageDataGenerator can help balance things out."
      ],
      "metadata": {
        "id": "QN8hrnXtJBHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks Deliverable 4 : Evaluating Diferent Models Using Transfer Learning"
      ],
      "metadata": {
        "id": "Xyc9gwq6uXdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download the subset of the Food-101 dataset. The subset is composed of 3 classes (apple_pie, caesar_salad, falafel). The size of the file is 154MB."
      ],
      "metadata": {
        "id": "3BSSENh1uoTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "from datetime import datetime\n",
        "\n",
        "current_date = date.today()\n",
        "print(\"Today's date:\", current_date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbQ67316m7uo",
        "outputId": "e83fa477-70e4-4cba-d1c3-a4db9bdbeb3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today's date: 2024-03-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "now1 = datetime.now()\n",
        "\n",
        "start_time = now1.strftime(\"%H:%M:%S\")\n",
        "print(\"Start Time =\", start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8INa1SOnFzf",
        "outputId": "8e57df50-02e3-47c1-a0c1-a43f74eeb0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Time = 17:05:22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install watermark tensorflow==2.3.* -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBq-ZOohnO6m",
        "outputId": "f34c1c56-6633-4fea-b1f8-9cac3a7b3d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.3.* (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.16.0rc0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.3.*\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import requests\n",
        "import glob\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import pprint\n",
        "import json\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "ZahQ1mtRnVji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "4fvPlhKknjDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "0CrOUUwfnnZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install watermark\n",
        "import watermark\n",
        "%load_ext watermark\n",
        "%reload_ext watermark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l1vce8Mnxjt",
        "outputId": "84a0d405-d775-415d-df64-9a4a8601dc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: watermark in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
            "Requirement already satisfied: ipython>=6.0 in /usr/local/lib/python3.10/dist-packages (from watermark) (7.34.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from watermark) (7.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from watermark) (67.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->watermark) (3.17.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.0->watermark) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.0->watermark) (0.2.13)\n",
            "The watermark extension is already loaded. To reload it, use:\n",
            "  %reload_ext watermark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -n -v -m -g -iv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhyHRZQfojhk",
        "outputId": "79e31d3a-951e-4363-e91c-69d363c54463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "Compiler    : GCC 11.4.0\n",
            "OS          : Linux\n",
            "Release     : 6.1.58+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "Git hash: \n",
            "\n",
            "watermark     : 2.4.3\n",
            "cv2           : 4.8.0\n",
            "tensorflow_hub: 0.16.1\n",
            "numpy         : 1.25.2\n",
            "splitfolders  : 0.5.1\n",
            "IPython       : 7.34.0\n",
            "PIL           : 9.4.0\n",
            "pathlib       : 1.0.1\n",
            "matplotlib    : 3.7.1\n",
            "keras         : 2.15.0\n",
            "requests      : 2.31.0\n",
            "tensorflow    : 2.15.0\n",
            "json          : 2.0.9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4mpPHLYaosFs",
        "outputId": "062404d0-d8d6-41ea-afd1-9b555ca965d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkhAqQtoo2EA",
        "outputId": "3503c624-ea3d-4e1e-a214-3a7b89ed1e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Project Folder\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPcBbEXFpNzx",
        "outputId": "5d36dcac-97d7-43a2-cba2-2438f69fc0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO-9xldKpUBY",
        "outputId": "432ffed0-d9f9-4726-ca27-146721258476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 4 root root 4096 Mar  6 16:56 project_food_dl/\n",
            "drwxr-xr-x 1 root root 4096 Mar  4 14:28 sample_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = \"project_food_dl\""
      ],
      "metadata": {
        "id": "ABCnE2MBpbcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sub-directory for the data\n",
        "# run this\n",
        "\n",
        "!mkdir -p {PROJECT_NAME}\n"
      ],
      "metadata": {
        "id": "40yKPdpmplLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17SJVE3Spt_r",
        "outputId": "09380220-d3f9-48ef-cc6c-e19dab80bc42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 4 root root 4096 Mar  6 16:56 project_food_dl/\n",
            "drwxr-xr-x 1 root root 4096 Mar  4 14:28 sample_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lF {PROJECT_NAME}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pygL03l6p1BA",
        "outputId": "2e108fc7-e343-4ec3-b61e-d5597740f3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Mar  6 17:02 artifacts/\n",
            "drwxr-xr-x 4 root root 4096 Mar  6 16:56 data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove log files from models\n",
        "!rm -rf {PROJECT_NAME}/artifacts"
      ],
      "metadata": {
        "id": "DijWgrBcp6MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {PROJECT_NAME}/data/food-101.tar.gz"
      ],
      "metadata": {
        "id": "DWD7MpYIp_fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f artifacts.zip"
      ],
      "metadata": {
        "id": "ObWN58kdqE3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sub-directory for data\n",
        "!mkdir -p {PROJECT_NAME}/data\n"
      ],
      "metadata": {
        "id": "D3mX-uK5qGc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {PROJECT_NAME} -lF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTIXzO5AqP-3",
        "outputId": "a5e07573-0804-43af-9f2f-084545944aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 4 root root 4096 Mar  6 16:56 data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sub-directory for artifacts\n",
        "!mkdir -p {PROJECT_NAME}/artifacts"
      ],
      "metadata": {
        "id": "UHnUTFQGqYhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {PROJECT_NAME} -lF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh2-aiyAqlU-",
        "outputId": "6c10a686-e7c4-4073-d601-1d3a70a22b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Mar  6 17:05 artifacts/\n",
            "drwxr-xr-x 4 root root 4096 Mar  6 16:56 data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data"
      ],
      "metadata": {
        "id": "eRqKuOW-qqOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer here code:\n",
        "! wget https://lp-prod-resources.s3-us-west-2.amazonaws.com/other/Deploying+a+Deep+Learning+Model+on+Web+and+Mobile+Applications+Using+TensorFlow/Food+101+-+Data+Subset.zip -P {PROJECT_NAME}/data\n"
      ],
      "metadata": {
        "id": "QsCF7N0Punt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9c7c55-b4d8-48a1-e0b1-9271bdbacd4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-06 17:05:30--  https://lp-prod-resources.s3-us-west-2.amazonaws.com/other/Deploying+a+Deep+Learning+Model+on+Web+and+Mobile+Applications+Using+TensorFlow/Food+101+-+Data+Subset.zip\n",
            "Resolving lp-prod-resources.s3-us-west-2.amazonaws.com (lp-prod-resources.s3-us-west-2.amazonaws.com)... 3.5.79.152, 3.5.82.144, 52.92.181.18, ...\n",
            "Connecting to lp-prod-resources.s3-us-west-2.amazonaws.com (lp-prod-resources.s3-us-west-2.amazonaws.com)|3.5.79.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153668842 (147M) [application/zip]\n",
            "Saving to: ‘project_food_dl/data/Food+101+-+Data+Subset.zip.2’\n",
            "\n",
            "Food+101+-+Data+Sub 100%[===================>] 146.55M  20.4MB/s    in 17s     \n",
            "\n",
            "2024-03-06 17:05:48 (8.50 MB/s) - ‘project_food_dl/data/Food+101+-+Data+Subset.zip.2’ saved [153668842/153668842]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unpack the data\n",
        "# run once, then comment out\n",
        "\n",
        "!unzip -q {PROJECT_NAME}/data/Food+101+-+Data+Subset.zip -d {PROJECT_NAME}/data"
      ],
      "metadata": {
        "id": "rs1DU4rfl9NA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120b7fb0-dca1-46e8-a63e-65ae03c99361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace project_food_dl/data/__MACOSX/._food-101-subset? [y]es, [n]o, [A]ll, [N]one, [r]ename: All\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {PROJECT_NAME} -lF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhC4u1jVmWql",
        "outputId": "f66e1e96-941f-41b4-ac78-e569cb2cb974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Mar  6 17:05 artifacts/\n",
            "drwxr-xr-x 4 root root 4096 Mar  6 17:05 data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = str(PROJECT_NAME)+\"/data/food-101-subset/images\"\n",
        "DATA_DIR = pathlib.Path(DATA_DIR)"
      ],
      "metadata": {
        "id": "6buR1JQmmh4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DATA_DIR:\", DATA_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFufROEQrP4y",
        "outputId": "94d5e727-6440-47ac-f8cd-f22e5bcafafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_DIR: project_food_dl/data/food-101-subset/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Look at Dataset"
      ],
      "metadata": {
        "id": "ApWcNDkJsWjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look at folder names\n",
        "!ls -lah {DATA_DIR}/ | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ykgNFVEsYWA",
        "outputId": "8bbe3710-bf88-45c2-e0a0-fa732175a127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 136K\n",
            "drwxr-xr-x 5 root root 4.0K Mar  6 17:06 .\n",
            "drwxr-xr-x 3 root root 4.0K Mar  6 17:06 ..\n",
            "drwxr-xr-x 2 root root  36K Mar  6 17:06 apple_pie\n",
            "drwxr-xr-x 2 root root  36K Mar  6 17:06 caesar_salad\n",
            "-rw-r--r-- 1 root root 6.1K Dec  5  2020 .DS_Store\n",
            "drwxr-xr-x 2 root root  36K Mar  6 17:06 falafel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = list(DATA_DIR.glob('*/*'))"
      ],
      "metadata": {
        "id": "VmnRYmg6rfRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loaded image paths:\")\n",
        "for image_path in images[:5]:\n",
        "  print(image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoOa-sS8rs4v",
        "outputId": "d56aed07-1918-4d77-9f10-0c019cb044f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded image paths:\n",
            "project_food_dl/data/food-101-subset/images/apple_pie/2698889.jpg\n",
            "project_food_dl/data/food-101-subset/images/apple_pie/3760078.jpg\n",
            "project_food_dl/data/food-101-subset/images/apple_pie/624715.jpg\n",
            "project_food_dl/data/food-101-subset/images/apple_pie/1456028.jpg\n",
            "project_food_dl/data/food-101-subset/images/apple_pie/1106961.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# look at fisrt five images in first image folder\n",
        "!ls {DATA_DIR}/apple_pie | head -5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svMxEbmsshw7",
        "outputId": "bd0b5bcd-fd91-4b84-97a2-01eb5f18870e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1005649.jpg\n",
            "1011328.jpg\n",
            "101251.jpg\n",
            "1014775.jpg\n",
            "1026328.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find out how many total images there are in database\n",
        "image_count = len(list(DATA_DIR.glob('*/*.jpg')))\n",
        "image_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxs7V4aXMlen",
        "outputId": "2910a044-0b88-44f3-c222-d148c5b92f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find out how many different classes there are\n",
        "ALL_CLASS_NAMES = sorted(np.array([item.name for item in DATA_DIR.glob('*')]))\n",
        "print(len(ALL_CLASS_NAMES))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNWOYUDwM0Nx",
        "outputId": "0fe568ab-deb9-4c04-b64f-371350118d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_CLASS_NAMES[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG8OFqyHNFPH",
        "outputId": "d6eae530-bdaf-4d18-c220-faa12932e037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.DS_Store', 'apple_pie', 'caesar_salad', 'falafel']"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CLASS_NAMES = ALL_CLASS_NAMES"
      ],
      "metadata": {
        "id": "f-8kMD63NI54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class1 = ALL_CLASS_NAMES[0]"
      ],
      "metadata": {
        "id": "XJrrvBOUNLq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display  # found out why is not displaying the images\n",
        "\n",
        "images = list(DATA_DIR.glob(f'{class1}/*'))\n",
        "\n",
        "for image_path in images[:2]:\n",
        "    # resize image\n",
        "    im = Image.open(str(image_path))\n",
        "    w, h = im.size\n",
        "    print('Image Size (w, h): ', w, \",\",  h)\n",
        "    print (image_path)\n",
        "    percent_resize = 0.5\n",
        "    im = im.resize((int(w*percent_resize), int(h*percent_resize)))\n",
        "    display.display(im)"
      ],
      "metadata": {
        "id": "egYpFsQHNuJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MkflDieVp_u1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up for Training Module"
      ],
      "metadata": {
        "id": "qvWmuj-akoj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ImageDataGenerator is used to create training and validation splits. It also has several builtin image preprocessing transformations.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"
      ],
      "metadata": {
        "id": "5ucPSHaMku6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
        "\n",
        "print(\"Number of classes we are training: \" ,len(USE_CLASS_NAMES))\n",
        "print(\"\\nList of classes\")\n",
        "list(USE_CLASS_NAMES)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdENzKMRj0-F",
        "outputId": "0f3a7175-6a5a-44c4-f6bd-2d0852958228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes we are training:  4\n",
            "\n",
            "List of classes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.DS_Store', 'apple_pie', 'caesar_salad', 'falafel']"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_data_generator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input ):\n",
        "  image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    validation_split=0.2,\n",
        "    preprocessing_function=preprocessing_function\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  # create a data generator object with options (location of images, batch size, option to shuffle, etc)\n",
        "  image_data_gen = image_generator.flow_from_directory(\n",
        "      directory=str(DATA_DIR),\n",
        "      batch_size=BATCH_SIZE,\n",
        "      shuffle=True,\n",
        "      target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "      classes = list(USE_CLASS_NAMES)\n",
        "      )\n",
        "\n",
        "  return image_data_gen"
      ],
      "metadata": {
        "id": "Px8D-B-tpPul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_data_gen = get_image_data_generator (preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_E3XoO-pQk9",
        "outputId": "b0835fa5-cfc4-48a1-d131-6182d029c898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save list of classes as classes.json"
      ],
      "metadata": {
        "id": "Rt9edvrkpcwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_data_gen.num_classes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGhPTVxdpa-2",
        "outputId": "21a75266-94fb-45a9-fb01-1d6c900cec3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_data_gen.class_indices.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AVhEa0op0XW",
        "outputId": "8bfb45ff-379e-4b20-ef65-d863ab30b564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['.DS_Store', 'apple_pie', 'caesar_salad', 'falafel'])"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_classes = list(image_data_gen.class_indices.keys())"
      ],
      "metadata": {
        "id": "1Aizv7DCpgQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_H73Ta-qB49",
        "outputId": "2da50367-fb48-4476-aae5-d77126f9d5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.DS_Store', 'apple_pie', 'caesar_salad', 'falafel']"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{PROJECT_NAME}/artifacts/classes.json\",'w') as f:\n",
        "  json.dump(list_of_classes,f)"
      ],
      "metadata": {
        "id": "eScAwDYMqHAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architectures\n",
        "\n",
        "## Model 1: VGG19(Baseline)"
      ],
      "metadata": {
        "id": "2LxdzfHtqJRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "?tf.keras.layers.Dense"
      ],
      "metadata": {
        "id": "-9nfV82yqREk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (IMG_HEIGHT, IMG_WIDTH)\n",
        "\n",
        "# Use VGG19 pretrained on ImageNet\n",
        "base_layers = tf.keras.applications.VGG19(weights='imagenet',include_top=False,input_shape=IMAGE_SHAPE+(3,) )\n",
        "\n",
        "# Add new layers to be finetuned\n",
        "# The last layer, is the classification layer and should match the number of classes in the dataset. The activation should be softmax\n",
        "clf = tf.keras.Sequential([\n",
        "    base_layers\n",
        "    , tf.keras.layers.GlobalAveragePooling2D()\n",
        "    , tf.keras.layers.Dense(1024, activation='relu')\n",
        "    , tf.keras.layers.Dense(image_data_gen.num_classes , name='classification', activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "VtY9-XGvqcI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjvB63Cyqf9c",
        "outputId": "30892a58-9dba-489c-9dc4-0dcaaf4bd967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
            "                                                                 \n",
            " global_average_pooling2d_2  (None, 512)               0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              525312    \n",
            "                                                                 \n",
            " classification (Dense)      (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20553796 (78.41 MB)\n",
            "Trainable params: 20553796 (78.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freezes the base layers\n",
        "base_layers.trainable = False"
      ],
      "metadata": {
        "id": "IrwJyrYNrDNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notice that after freezing the base layers, the non trainable params are equal to the number of parameters in the base layer\n",
        "clf.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoFRi-1DrD-m",
        "outputId": "5869079a-01e2-4223-de4b-c0d93a44a631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
            "                                                                 \n",
            " global_average_pooling2d_2  (None, 512)               0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              525312    \n",
            "                                                                 \n",
            " classification (Dense)      (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20553796 (78.41 MB)\n",
            "Trainable params: 529412 (2.02 MB)\n",
            "Non-trainable params: 20024384 (76.39 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to use Adam optimizer , cross entropy loss, and track accuracy.\n",
        "# Since the dataset has multiple classes, we are using cross entropy loss.\n",
        "clf.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss='categorical_crossentropy' ,\n",
        "  metrics=['acc'])"
      ],
      "metadata": {
        "id": "qnZOdscSrNCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Results"
      ],
      "metadata": {
        "id": "GLD8cbNtrPjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model for 5 epochs\n",
        "%%time\n",
        "image_data_gen = get_image_data_generator (preprocessing_function=tf.keras.applications.vgg19.preprocess_input)\n",
        "history = clf.fit(image_data_gen\n",
        "                        ,epochs=5\n",
        "                        ,workers=8\n",
        "                        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKpDUg4brRjn",
        "outputId": "d2f93f89-b0ca-4463-be46-983162319679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 images belonging to 4 classes.\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 17s 161ms/step - loss: 1.3699 - acc: 0.8233\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 16s 163ms/step - loss: 0.1877 - acc: 0.9420\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 16s 160ms/step - loss: 0.0723 - acc: 0.9763\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 16s 158ms/step - loss: 0.0285 - acc: 0.9937\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 15s 158ms/step - loss: 0.0155 - acc: 0.9977\n",
            "CPU times: user 1min 21s, sys: 5.41 s, total: 1min 26s\n",
            "Wall time: 1min 30s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "iUb2qCkurgu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model as `h5` format\n",
        "export_path = str(PROJECT_NAME)+\"/artifacts/model_VGG19.h5\"\n",
        "export_path\n",
        "clf.save(export_path, save_format='h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi2jx5mOriti",
        "outputId": "f74cd811-d734-453d-92b3-287f0e7879d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2: ResNet50\n",
        "\n",
        "On your own, train a model using ResNet50"
      ],
      "metadata": {
        "id": "e1WPeVZnsJCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders matplotlib opencv-python spicy"
      ],
      "metadata": {
        "id": "BZjoMdXEsSGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d88846-4006-4b73-d2ae-85b4dd7cf4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: spicy in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spicy) (1.11.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display, transform, read, split ...\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import os\n",
        "import splitfolders\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# image processing\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "\n",
        "# model / neural network\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input"
      ],
      "metadata": {
        "id": "SEgEUBuLnyO-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}